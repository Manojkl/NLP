{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d757077c2e9b853138564450eb0dd923",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>. If this does not work there is a submission slot on LEA.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to three people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID (LEA username) of each team member into the variables. \n",
    "If you work alone please fill the first variable only.\n",
    "'''\n",
    "member1 = 'mkolpe2s'\n",
    "member2 = 'agomez2s'\n",
    "member3 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b094dbf2483abc170896d43f9ff7263",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction to spaCy\n",
    "\n",
    "SpaCy is a tool that does tokenization, parsing, tagging and named entity regocnition (among other things).\n",
    "\n",
    "When we parse a document via spaCy, we get an object that holds sentences and tokens, as well as their POS tags, dependency relations and so on.\n",
    "\n",
    "Look at the next cell for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3d2e3cdb35d0a32e7027e215d79eb91",
     "grade": false,
     "grade_id": "intro_example",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy PROPN nsubj False\n",
      "is AUX ROOT True\n",
      "capable ADJ acomp False\n",
      "of ADP prep True\n",
      "    SPACE  False\n",
      "tagging NOUN pobj False\n",
      ", PUNCT punct False\n",
      "parsing VERB conj False\n",
      "and CCONJ cc True\n",
      "annotating VERB conj False\n",
      "text NOUN dobj False\n",
      ". PUNCT punct False\n",
      "It PRON nsubj True\n",
      "recognizes VERB ROOT False\n",
      "sentences NOUN dobj False\n",
      "and CCONJ cc True\n",
      "stop VERB conj False\n",
      "words NOUN dobj False\n",
      ". PUNCT punct False\n",
      "------------------------------\n",
      "The nouns and proper nouns in this text are:\n",
      "SpaCy\n",
      "tagging\n",
      "text\n",
      "sentences\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = 'SpaCy is capable of    tagging, parsing and annotating text. It recognizes sentences and stop words.'\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# For every sentence\n",
    "for sent in doc.sents:\n",
    "    # For every token\n",
    "    for token in sent:\n",
    "        # Print the token itself, the pos tag, \n",
    "        # dependency tag and whether spacy thinks this is a stop word\n",
    "        print(token, token.pos_, token.dep_, token.is_stop)\n",
    "        \n",
    "print('-'*30)\n",
    "print('The nouns and proper nouns in this text are:')\n",
    "# Print only the nouns:\n",
    "for token in doc:\n",
    "    if token.pos_ in ['NOUN', 'PROPN']:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77ff6dadb1f9f62fb537c233ddd59edf",
     "grade": false,
     "grade_id": "splitting_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Splitting text into sentences\n",
    "\n",
    "You are given the text in the next cell.\n",
    "\n",
    "```\n",
    "text = '''\n",
    "This is a sentence. \n",
    "Mr. A. said this was another! \n",
    "But is this a sentence? \n",
    "The abbreviation Merch. means merchant(s).\n",
    "At certain univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "```\n",
    "\n",
    "Use spaCy to split this into sentences. Store the resulting sentences (each as a **single** string) in the list ```sentences```. Make sure to convert the tokens to strings (e.g. via str(token))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4cfe8bbff19b15e5a363a436743ff24",
     "grade": false,
     "grade_id": "splitting",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is a sentence.\n",
      ".\n",
      "Mr. A. said this was another! \n",
      "\n",
      ".\n",
      "But is this a sentence?\n",
      ".\n",
      "The abbreviation Merch. means merchant(s).\n",
      "\n",
      ".\n",
      "At certain Univ.\n",
      ".\n",
      "in the U.S. and U.K. they study NLP.\n",
      "\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "sentences = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "doc = nlp(text)\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    sentences.append(str(sentence))\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print('.')\n",
    "    assert type(sentence) == str, 'You need to convert this to a single string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "707643bac503e6d9be2f85c773854515",
     "grade": true,
     "grade_id": "test_splitting",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c90efd1d0916557a1abc2e505e02e4b9",
     "grade": false,
     "grade_id": "pos_cluster_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Cluster the text by POS tag\n",
    "\n",
    "Next we want to cluster the text by the corresponding part-of-speech (POS) tags. \n",
    "\n",
    "The result should be a dictionary ```pos_tags``` where the keys are the POS tags and the values are lists of words with those POS tags. Make sure your words are converted to **strings**.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "pos_tags['VERB'] # Output: ['said', 'means', 'study']\n",
    "pos_tags['ADJ']  # Output: ['certain']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1704ce8495f5a1380141f253b16b857e",
     "grade": false,
     "grade_id": "pos_cluster",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words with the POS tag ADJ are ['certain'].\n",
      "The words with the POS tag PRON are ['they'].\n",
      "The words with the POS tag DET are ['This', 'a', 'this', 'another', 'this', 'a', 'The', 'the'].\n",
      "The words with the POS tag PUNCT are ['.', '!', '?', '.', ')', '.', '.', '.'].\n",
      "The words with the POS tag SPACE are ['\\n', '\\n', '\\n', '\\n'].\n",
      "The words with the POS tag CCONJ are ['But', 'and'].\n",
      "The words with the POS tag VERB are ['said', 'means', 'study'].\n",
      "The words with the POS tag ADP are ['At', 'in'].\n",
      "The words with the POS tag PROPN are ['Mr.', 'A.', 'Merch', 'merchant(s', 'Univ', 'U.S.', 'U.K.', 'NLP'].\n",
      "The words with the POS tag NOUN are ['sentence', 'sentence', 'abbreviation'].\n",
      "The words with the POS tag AUX are ['is', 'was', 'is'].\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "pos_tags = dict()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    if str(token.pos_) in pos_tags:\n",
    "        pos_tags[str(token.pos_)].append(token.text)\n",
    "    else:\n",
    "        pos_tags[str(token.pos_)] = [token.text]\n",
    "\n",
    "\n",
    "\n",
    "for key in pos_tags:\n",
    "    print('The words with the POS tag {} are {}.'.format(key, pos_tags[key]))\n",
    "    for token in pos_tags[key]:\n",
    "        assert type(token) == str, 'Each token should be a string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c7fc8fae3ff404aaacaa0db50a5e3f6",
     "grade": true,
     "grade_id": "test_pos_cluster",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "042b9e4e7861106c0c3dc03614a56b42",
     "grade": false,
     "grade_id": "stopwords_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Stop word removal\n",
    "\n",
    "Stop words are words that appear often in a language and don't hold much meaning for a NLP task. Examples are the words ```a, to, the, this, has, ...```. This depends on the task and domain you are working on.\n",
    "\n",
    "SpaCy has its own internal list of stop words. Use spaCy to remove all stop words from the given text. Store your result as a **single string** in the variable ```stopwords_removed```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ae2be296324792bf1c5fe64d7bdfc5e",
     "grade": false,
     "grade_id": "stopwords",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence. Mr. A. said ! \n",
      "sentence? abbreviation Merch. means merchant(s).\n",
      "certain Univ. U.S. U.K. study NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "stopwords_removed = ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "doc = nlp(text)\n",
    "    \n",
    "for token in doc:\n",
    "    if(not token.is_stop):\n",
    "        stopwords_removed += token.text\n",
    "        if(token.whitespace_):\n",
    "            stopwords_removed += token.whitespace_\n",
    "\n",
    "print(stopwords_removed)\n",
    "assert type(stopwords_removed) == str, 'Your answer should be a single string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c93999af4c8e248316f9364bcdf1056",
     "grade": true,
     "grade_id": "test_stopwords",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee51e6614ec74383dbc881fc6b5a3a22",
     "grade": false,
     "grade_id": "dependency_tree_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Dependency Tree\n",
    "\n",
    "We now want to use spaCy to visualize the dependency tree of a certain sentence. Look at the Jupyter Example on the [spaCy website](https://spacy.io/usage/visualizers/). Render the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cdfc04fbcd5a42c4815f3eb516a82a3",
     "grade": true,
     "grade_id": "dependency_tree",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a2b6bbd6635c46239aae09f4e711892f-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Dependency</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Parsing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">helpful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">many</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">tasks.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a2b6bbd6635c46239aae09f4e711892f-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a2b6bbd6635c46239aae09f4e711892f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a2b6bbd6635c46239aae09f4e711892f-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a2b6bbd6635c46239aae09f4e711892f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a2b6bbd6635c46239aae09f4e711892f-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a2b6bbd6635c46239aae09f4e711892f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a2b6bbd6635c46239aae09f4e711892f-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a2b6bbd6635c46239aae09f4e711892f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a2b6bbd6635c46239aae09f4e711892f-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a2b6bbd6635c46239aae09f4e711892f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a2b6bbd6635c46239aae09f4e711892f-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a2b6bbd6635c46239aae09f4e711892f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = 'Dependency Parsing is helpful for many tasks.'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b88d826d04d3a093274f010b5f1e613",
     "grade": false,
     "grade_id": "dependency_parsing_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5 Dependency Parsing\n",
    "\n",
    "Use spaCy to extract all subjects and objects from the text. We define a subject as any word that has ```subj``` in its dependency tag (e.g. ```nsubj```, ```nsubjpass```, ...). Similarly we define an object as any token that has ```obj``` in its dependency tag (e.g. ```dobj```, ```pobj```, etc.).\n",
    "\n",
    "For each sentence extract the subject, root node ```ROOT``` of the tree and object and store them as a single string in a list. Name this list ```subj_obj```.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "text = 'Learning multiple ways of representing text is cool. We can access parts of the sentence with dependency tags.'\n",
    "\n",
    "subj_obj = ['Learning ways text is', 'We access parts sentence tags']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78d8922b0ac7a8c88b0ceffd0fc52ce5",
     "grade": false,
     "grade_id": "dependency_parsing",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is \n",
      "A. said this \n",
      "is this \n",
      "abbreviation means \n",
      "At Univ\n",
      "U.S. they study NLP\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "subj_obj = []\n",
    "# YOUR CODE HERE\n",
    "doc = nlp(text)\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    sen = ''\n",
    "    for token in sentence:\n",
    "        if 'subj' in token.dep_ or 'obj' in token.dep_ or 'ROOT' in token.dep_:\n",
    "            sen += token.text\n",
    "            if(token.whitespace_):\n",
    "                sen += token.whitespace_\n",
    "            \n",
    "    subj_obj.append(sen)\n",
    "\n",
    "for cleaned_sent in subj_obj:\n",
    "    print(cleaned_sent)\n",
    "    assert type(cleaned_sent) == str, 'Each cleaned sentence should be a string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9223b52b2473c2a6b9257f8dcbb02329",
     "grade": true,
     "grade_id": "test_dependency_parsing",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "659739a04376dd68b7e18a039be83e4a",
     "grade": false,
     "grade_id": "poskeyword_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Keyword Extraction\n",
    "\n",
    "In this assignment we want to write a keyword extractor. There are several methods of which we want to explore a few.\n",
    "\n",
    "We want to extract keywords from Wikipedia articles about ```Natural language processing```.\n",
    "\n",
    "## 2.1 POS tag based extraction\n",
    "\n",
    "When we look at keywords we realize that they are often combinations of nouns and adjectives. The idea is to find all sequences of nouns and adjectives in a corpus and count them. The $n$ most frequent ones are then our keywords.\n",
    "\n",
    "A keyword by this definition is any combination of nouns (NOUN) and adjectives (ADJ) that ends in a noun. We also count proper nouns (PROPN) as nouns.\n",
    "\n",
    "### 2.1.1 POSKeywordExtractor\n",
    "\n",
    "Please complete the function ```keywords``` in the class ```POSKeywordExtractor```.\n",
    "\n",
    "You are given the file ```corpus.txt```, which has the raw text from all top-level pages under the category ```Natural language processing```. Use this for extracting your keywords.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "Let us look at the definition of an index term or keyword from Wikipedia. Here I highlighted all combinations of nouns and adjectives that end in a noun. All the highlighted words are potential keywords.\n",
    "\n",
    "An **index term**, **subject term**, **subject heading**, or **descriptor**, in **information retrieval**, is a **term** that captures the **essence** of the **topic** of a **document**. **Index terms** make up a **controlled vocabulary** for **use** in **bibliographic records**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76657ec6872d35a7de8fb08762118e3f",
     "grade": false,
     "grade_id": "poskeywords",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('words',) appears 349 times.\n",
      "The keyword ('text',) appears 345 times.\n",
      "The keyword ('example',) appears 267 times.\n",
      "The keyword ('word',) appears 239 times.\n",
      "The keyword ('natural', 'language', 'processing') appears 180 times.\n",
      "The keyword ('references',) appears 167 times.\n",
      "The keyword ('documents',) appears 160 times.\n",
      "The keyword ('language',) appears 157 times.\n",
      "The keyword ('information',) appears 152 times.\n",
      "The keyword ('systems',) appears 147 times.\n",
      "The keyword ('set',) appears 135 times.\n",
      "The keyword ('system',) appears 130 times.\n",
      "The keyword ('sentence',) appears 114 times.\n",
      "The keyword ('number',) appears 113 times.\n",
      "The keyword ('context',) appears 110 times.\n",
      "CPU times: user 5.28 s, sys: 917 ms, total: 6.19 s\n",
      "Wall time: 6.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "class POSKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "                          Example: [(('potato'), 12), (('potato', 'harvesting'), 9), ...]\n",
    "        '''\n",
    "        #text ='An index term, subject term, subject heading, or descriptor, in information retrieval, is a term that captures the essence of the topic of a document. Index terms make up a controlled vocabulary for use in bibliographic records.'\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        keywords = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        preliminar_keywords = []\n",
    "        curr_keyword = []\n",
    "        for token in doc:\n",
    "            if(token.pos_ == 'NOUN' or token.pos_ == 'PNOUN' or token.pos_ =='ADJ'):\n",
    "                curr_keyword.append(token)\n",
    "                #print(token.pos_)\n",
    "                \n",
    "            elif(len(curr_keyword) >= min_words):\n",
    "                if(curr_keyword[-1].pos_ == 'NOUN' or curr_keyword[-1].pos_ == 'PNOUN'):\n",
    "                    new_keyword = [t.text for t in curr_keyword]\n",
    "                    preliminar_keywords.append(' '.join(new_keyword))\n",
    "                curr_keyword = []\n",
    "                \n",
    "        #print(preliminar_keywords)\n",
    "        word_freq = Counter(preliminar_keywords)\n",
    "        sorted_keywords = word_freq.most_common(n_keywords)\n",
    "        \n",
    "        for word in sorted_keywords:\n",
    "            keywords.append((tuple(word[0].split(' ')), word[1]))\n",
    "\n",
    "        return keywords\n",
    "    \n",
    "with open('corpus.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('text',) appears 330 times.\n",
    "The keyword ('words',) appears 317 times.\n",
    "The keyword ('example',) appears 255 times.\n",
    "The keyword ('word',) appears 217 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))#s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cf524bee04e1833db8060b601e803af",
     "grade": true,
     "grade_id": "test_poskeywords",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "faf367844aa2332d46c651da9a00233d",
     "grade": false,
     "grade_id": "poskeywords_params_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1.2 Testing parameters\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cba0ac25130b2420ca8651fa3702e270",
     "grade": false,
     "grade_id": "poskeywords_params",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('natural', 'language', 'processing') appears 116 times.\n",
      "The keyword ('computational', 'linguistics') appears 58 times.\n",
      "The keyword ('machine', 'translation') appears 48 times.\n",
      "The keyword ('external', 'links') appears 47 times.\n",
      "The keyword ('natural', 'language') appears 46 times.\n",
      "The keyword ('sentiment', 'analysis') appears 43 times.\n",
      "The keyword ('references', 'external', 'links') appears 41 times.\n",
      "The keyword ('text', 'mining') appears 41 times.\n",
      "The keyword ('word', 'sense', 'disambiguation') appears 36 times.\n",
      "The keyword ('artificial', 'intelligence') appears 28 times.\n",
      "The keyword ('machine', 'learning') appears 28 times.\n",
      "The keyword ('natural', 'language', 'understanding') appears 28 times.\n",
      "The keyword ('information', 'extraction') appears 24 times.\n",
      "The keyword ('customer', 'card') appears 22 times.\n",
      "The keyword ('speech', 'recognition') appears 21 times.\n"
     ]
    }
   ],
   "source": [
    "keywords_2 = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "keywords_2 = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=2)\n",
    "for keyword in keywords_2:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78869f2232ec1e8a6253f2b3967d6775",
     "grade": true,
     "grade_id": "test_poskeywords_params",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9623e058d7b5c9fb2ebe52fa3a788958",
     "grade": false,
     "grade_id": "stopkeywords_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 Stop word based extraction\n",
    "\n",
    "Another approach to extract keywords is by splitting the text at the stop words. Then we count these potential keywords and output the top $n$ keywords. Make sure to only include words proper words. Here we define proper words as those words that match the regular expression ```r'\\b(\\W+|\\w+)\\b'```. \n",
    "\n",
    "### 2.2.1 StopWordKeywordExtractor\n",
    "\n",
    "Complete the function ```keywords``` in the class ```StopWordKeywordExtractor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8889a80e2eb27a5241fa8050735c028a",
     "grade": false,
     "grade_id": "stopkeywords",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('words',) appears 273 times.\n",
      "The keyword ('text',) appears 263 times.\n",
      "The keyword ('example',) appears 257 times.\n",
      "The keyword ('word',) appears 201 times.\n",
      "The keyword ('references',) appears 184 times.\n",
      "The keyword ('natural', 'language', 'processing') appears 165 times.\n",
      "The keyword ('n',) appears 160 times.\n",
      "The keyword ('use',) appears 151 times.\n",
      "The keyword ('set',) appears 144 times.\n",
      "The keyword ('language',) appears 123 times.\n",
      "The keyword ('t',) appears 120 times.\n",
      "The keyword ('documents',) appears 118 times.\n",
      "The keyword ('based',) appears 115 times.\n",
      "The keyword ('1',) appears 115 times.\n",
      "The keyword ('number',) appears 106 times.\n",
      "CPU times: user 5.46 s, sys: 817 ms, total: 6.28 s\n",
      "Wall time: 6.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "class StopWordKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    def is_proper_word(self, token:str) -> bool:\n",
    "        '''\n",
    "        Checks if the word is a proper word by our definition\n",
    "        \n",
    "        Arguments:\n",
    "            token     -- The token as a string\n",
    "        Return:\n",
    "            is_proper -- True / False\n",
    "        '''\n",
    "        match = re.search(r'\\b(\\W+|\\w+)\\b', token)\n",
    "        if(match):\n",
    "            return token == match.group(0)\n",
    "        else:\n",
    "            return False \n",
    "    \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to retu[rn\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "                          Example: [(('potato'), 12), (('potato', 'harvesting'), 9), ...]\n",
    "        '''\n",
    "        doc = self.nlp(text)\n",
    "        keywords = []\n",
    "        # YOUR CODE HERE\n",
    "        curr_keyword = []\n",
    "        preliminar_keywords = []\n",
    "        \n",
    "        for token in doc:\n",
    "            self.is_proper_word(token.text)\n",
    "            if(not token.is_stop and self.is_proper_word(token.text)):\n",
    "                #print(token)\n",
    "                curr_keyword.append(token.text)\n",
    "                \n",
    "            elif(len(curr_keyword) >= min_words):\n",
    "                preliminar_keywords.append(' '.join(curr_keyword))\n",
    "                curr_keyword = []\n",
    "                \n",
    "        word_freq = Counter(preliminar_keywords)\n",
    "        sorted_keywords = word_freq.most_common(n_keywords)\n",
    "        \n",
    "        for word in sorted_keywords:\n",
    "            keywords.append((tuple(word[0].split(' ')), word[1]))\n",
    "\n",
    "        return keywords\n",
    "        \n",
    "with open('corpus.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = StopWordKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('words',) appears 273 times.\n",
    "The keyword ('text',) appears 263 times.\n",
    "The keyword ('example',) appears 257 times.\n",
    "The keyword ('word',) appears 201 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "626be40b6cd83a9d976e3be2f3f74c4e",
     "grade": true,
     "grade_id": "test_stopkeywords",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68b93f319c96f297d09b1820b1478b6c",
     "grade": false,
     "grade_id": "stopwordkeywords_params_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2.2 Testing parameters\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cba16ca02e4237d2b3f486cfdc5c4161",
     "grade": false,
     "grade_id": "stopwordkeywords_params",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keyword ('natural', 'language', 'processing') appears 115 times.\n",
      "The keyword ('computational', 'linguistics') appears 60 times.\n",
      "The keyword ('references', 'external', 'links') appears 49 times.\n",
      "The keyword ('information', 'retrieval') appears 49 times.\n",
      "The keyword ('machine', 'translation') appears 42 times.\n",
      "The keyword ('external', 'links') appears 42 times.\n",
      "The keyword ('text', 'mining') appears 37 times.\n",
      "The keyword ('word', 'sense', 'disambiguation') appears 36 times.\n",
      "The keyword ('machine', 'learning') appears 34 times.\n",
      "The keyword ('natural', 'language') appears 30 times.\n",
      "The keyword ('artificial', 'intelligence') appears 29 times.\n",
      "The keyword ('sentiment', 'analysis') appears 28 times.\n",
      "The keyword ('natural', 'language', 'understanding') appears 25 times.\n",
      "The keyword ('speech', 'recognition') appears 23 times.\n",
      "The keyword ('association', 'computational', 'linguistics') appears 22 times.\n"
     ]
    }
   ],
   "source": [
    "keywords_2 = StopWordKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=2)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for keyword in keywords_2:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ee4fe07b305c2fb3de2b15c636810e6",
     "grade": true,
     "grade_id": "test_stopwordkeywords_params",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
