{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d757077c2e9b853138564450eb0dd923",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Wednesday at 15:00</b>. If this does not work there is a submission slot on LEA.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to three people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID (LEA username) of each team member into the variables. \n",
    "If you work alone please fill the first variable only.\n",
    "'''\n",
    "member1 = ''\n",
    "member2 = ''\n",
    "member3 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b094dbf2483abc170896d43f9ff7263",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction to spaCy\n",
    "\n",
    "SpaCy is a tool that does tokenization, parsing, tagging and named entity regocnition (among other things).\n",
    "\n",
    "When we parse a document via spaCy, we get an object that holds sentences and tokens, as well as their POS tags, dependency relations and so on.\n",
    "\n",
    "Look at the next cell for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3d2e3cdb35d0a32e7027e215d79eb91",
     "grade": false,
     "grade_id": "intro_example",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = 'SpaCy is capable of    tagging, parsing and annotating text. It recognizes sentences and stop words.'\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# For every sentence\n",
    "for sent in doc.sents:\n",
    "    # For every token\n",
    "    for token in sent:\n",
    "        # Print the token itself, the pos tag, \n",
    "        # dependency tag and whether spacy thinks this is a stop word\n",
    "        print(token, token.pos_, token.dep_, token.is_stop)\n",
    "        \n",
    "print('-'*30)\n",
    "print('The nouns and proper nouns in this text are:')\n",
    "# Print only the nouns:\n",
    "for token in doc:\n",
    "    if token.pos_ in ['NOUN', 'PROPN']:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77ff6dadb1f9f62fb537c233ddd59edf",
     "grade": false,
     "grade_id": "splitting_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Splitting text into sentences\n",
    "\n",
    "You are given the text in the next cell.\n",
    "\n",
    "```\n",
    "text = '''\n",
    "This is a sentence. \n",
    "Mr. A. said this was another! \n",
    "But is this a sentence? \n",
    "The abbreviation Merch. means merchant(s).\n",
    "At certain univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "```\n",
    "\n",
    "Use spaCy to split this into sentences. Store the resulting sentences (each as a **single** string) in the list ```sentences```. Make sure to convert the tokens to strings (e.g. via str(token))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4cfe8bbff19b15e5a363a436743ff24",
     "grade": false,
     "grade_id": "splitting",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "sentences = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print('.')\n",
    "    assert type(sentence) == str, 'You need to convert this to a single string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "707643bac503e6d9be2f85c773854515",
     "grade": true,
     "grade_id": "test_splitting",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c90efd1d0916557a1abc2e505e02e4b9",
     "grade": false,
     "grade_id": "pos_cluster_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Cluster the text by POS tag\n",
    "\n",
    "Next we want to cluster the text by the corresponding part-of-speech (POS) tags. \n",
    "\n",
    "The result should be a dictionary ```pos_tags``` where the keys are the POS tags and the values are lists of words with those POS tags. Make sure your words are converted to **strings**.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "pos_tags['VERB'] # Output: ['said', 'means', 'study']\n",
    "pos_tags['ADJ']  # Output: ['certain']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1704ce8495f5a1380141f253b16b857e",
     "grade": false,
     "grade_id": "pos_cluster",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "pos_tags = dict()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for key in pos_tags:\n",
    "    print('The words with the POS tag {} are {}.'.format(key, pos_tags[key]))\n",
    "    for token in pos_tags[key]:\n",
    "        assert type(token) == str, 'Each token should be a string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c7fc8fae3ff404aaacaa0db50a5e3f6",
     "grade": true,
     "grade_id": "test_pos_cluster",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "042b9e4e7861106c0c3dc03614a56b42",
     "grade": false,
     "grade_id": "stopwords_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Stop word removal\n",
    "\n",
    "Stop words are words that appear often in a language and don't hold much meaning for a NLP task. Examples are the words ```a, to, the, this, has, ...```. This depends on the task and domain you are working on.\n",
    "\n",
    "SpaCy has its own internal list of stop words. Use spaCy to remove all stop words from the given text. Store your result as a **single string** in the variable ```stopwords_removed```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ae2be296324792bf1c5fe64d7bdfc5e",
     "grade": false,
     "grade_id": "stopwords",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "stopwords_removed = ''\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(stopwords_removed)\n",
    "assert type(stopwords_removed) == str, 'Your answer should be a single string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c93999af4c8e248316f9364bcdf1056",
     "grade": true,
     "grade_id": "test_stopwords",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee51e6614ec74383dbc881fc6b5a3a22",
     "grade": false,
     "grade_id": "dependency_tree_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Dependency Tree\n",
    "\n",
    "We now want to use spaCy to visualize the dependency tree of a certain sentence. Look at the Jupyter Example on the [spaCy website](https://spacy.io/usage/visualizers/). Render the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cdfc04fbcd5a42c4815f3eb516a82a3",
     "grade": true,
     "grade_id": "dependency_tree",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = 'Dependency Parsing is helpful for many tasks.'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b88d826d04d3a093274f010b5f1e613",
     "grade": false,
     "grade_id": "dependency_parsing_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5 Dependency Parsing\n",
    "\n",
    "Use spaCy to extract all subjects and objects from the text. We define a subject as any word that has ```subj``` in its dependency tag (e.g. ```nsubj```, ```nsubjpass```, ...). Similarly we define an object as any token that has ```obj``` in its dependency tag (e.g. ```dobj```, ```pobj```, etc.).\n",
    "\n",
    "For each sentence extract the subject, root node ```ROOT``` of the tree and object and store them as a single string in a list. Name this list ```subj_obj```.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "```\n",
    "text = 'Learning multiple ways of representing text is cool. We can access parts of the sentence with dependency tags.'\n",
    "\n",
    "subj_obj = ['Learning ways text is', 'We access parts sentence tags']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78d8922b0ac7a8c88b0ceffd0fc52ce5",
     "grade": false,
     "grade_id": "dependency_parsing",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "This is a sentence. Mr. A. said this was another! \n",
    "But is this a sentence? The abbreviation Merch. means merchant(s).\n",
    "At certain Univ. in the U.S. and U.K. they study NLP.\n",
    "'''\n",
    "\n",
    "subj_obj = []\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "for cleaned_sent in subj_obj:\n",
    "    print(cleaned_sent)\n",
    "    assert type(cleaned_sent) == str, 'Each cleaned sentence should be a string!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9223b52b2473c2a6b9257f8dcbb02329",
     "grade": true,
     "grade_id": "test_dependency_parsing",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "659739a04376dd68b7e18a039be83e4a",
     "grade": false,
     "grade_id": "poskeyword_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Keyword Extraction\n",
    "\n",
    "In this assignment we want to write a keyword extractor. There are several methods of which we want to explore a few.\n",
    "\n",
    "We want to extract keywords from Wikipedia articles about ```Natural language processing```.\n",
    "\n",
    "## 2.1 POS tag based extraction\n",
    "\n",
    "When we look at keywords we realize that they are often combinations of nouns and adjectives. The idea is to find all sequences of nouns and adjectives in a corpus and count them. The $n$ most frequent ones are then our keywords.\n",
    "\n",
    "A keyword by this definition is any combination of nouns (NOUN) and adjectives (ADJ) that ends in a noun. We also count proper nouns (PROPN) as nouns.\n",
    "\n",
    "### 2.1.1 POSKeywordExtractor\n",
    "\n",
    "Please complete the function ```keywords``` in the class ```POSKeywordExtractor```.\n",
    "\n",
    "You are given the file ```corpus.txt```, which has the raw text from all top-level pages under the category ```Natural language processing```. Use this for extracting your keywords.\n",
    "\n",
    "*Example:*\n",
    "\n",
    "Let us look at the definition of an index term or keyword from Wikipedia. Here I highlighted all combinations of nouns and adjectives that end in a noun. All the highlighted words are potential keywords.\n",
    "\n",
    "An **index term**, **subject term**, **subject heading**, or **descriptor**, in **information retrieval**, is a **term** that captures the **essence** of the **topic** of a **document**. **Index terms** make up a **controlled vocabulary** for **use** in **bibliographic records**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76657ec6872d35a7de8fb08762118e3f",
     "grade": false,
     "grade_id": "poskeywords",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "\n",
    "class POSKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "    \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "                          Example: [(('potato'), 12), (('potato', 'harvesting'), 9), ...]\n",
    "        '''\n",
    "        doc = self.nlp(text)\n",
    "        keywords = []\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return keywords\n",
    "    \n",
    "with open('corpus.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('text',) appears 330 times.\n",
    "The keyword ('words',) appears 317 times.\n",
    "The keyword ('example',) appears 255 times.\n",
    "The keyword ('word',) appears 217 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cf524bee04e1833db8060b601e803af",
     "grade": true,
     "grade_id": "test_poskeywords",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "faf367844aa2332d46c651da9a00233d",
     "grade": false,
     "grade_id": "poskeywords_params_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1.2 Testing parameters\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cba0ac25130b2420ca8651fa3702e270",
     "grade": false,
     "grade_id": "poskeywords_params",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "keywords_2 = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78869f2232ec1e8a6253f2b3967d6775",
     "grade": true,
     "grade_id": "test_poskeywords_params",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9623e058d7b5c9fb2ebe52fa3a788958",
     "grade": false,
     "grade_id": "stopkeywords_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 Stop word based extraction\n",
    "\n",
    "Another approach to extract keywords is by splitting the text at the stop words. Then we count these potential keywords and output the top $n$ keywords. Make sure to only include words proper words. Here we define proper words as those words that match the regular expression ```r'\\b(\\W+|\\w+)\\b'```. \n",
    "\n",
    "### 2.2.1 StopWordKeywordExtractor\n",
    "\n",
    "Complete the function ```keywords``` in the class ```StopWordKeywordExtractor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8889a80e2eb27a5241fa8050735c028a",
     "grade": false,
     "grade_id": "stopkeywords",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "class StopWordKeywordExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    def is_proper_word(self, token:str) -> bool:\n",
    "        '''\n",
    "        Checks if the word is a proper word by our definition\n",
    "        \n",
    "        Arguments:\n",
    "            token     -- The token as a string\n",
    "        Return:\n",
    "            is_proper -- True / False\n",
    "        '''\n",
    "        match = re.search(r'\\b(\\W+|\\w+)\\b', token)\n",
    "        return match and token == match[0] \n",
    "    \n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text       -- the raw text from which to extract keywords\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "                          Example: [(('potato'), 12), (('potato', 'harvesting'), 9), ...]\n",
    "        '''\n",
    "        doc = self.nlp(text)\n",
    "        keywords = []\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return keywords\n",
    "        \n",
    "with open('corpus.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = StopWordKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('words',) appears 273 times.\n",
    "The keyword ('text',) appears 263 times.\n",
    "The keyword ('example',) appears 257 times.\n",
    "The keyword ('word',) appears 201 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "626be40b6cd83a9d976e3be2f3f74c4e",
     "grade": true,
     "grade_id": "test_stopkeywords",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68b93f319c96f297d09b1820b1478b6c",
     "grade": false,
     "grade_id": "stopwordkeywords_params_description",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2.2 Testing parameters\n",
    "\n",
    "Rerun the keyword extrator with a minimum word count of ```min_words=2``` and a keyword count of ```n_keywords=15```.\n",
    "\n",
    "Store this in the variable ```keywords_2```. Print the result.\n",
    "\n",
    "Make sure to convert the input text to lower case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cba16ca02e4237d2b3f486cfdc5c4161",
     "grade": false,
     "grade_id": "stopwordkeywords_params",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "keywords_2 = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ee4fe07b305c2fb3de2b15c636810e6",
     "grade": true,
     "grade_id": "test_stopwordkeywords_params",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
